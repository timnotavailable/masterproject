{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ef8b6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading and Labelling: 100%|█████████████████████████████████████████████████████████| 234/234 [03:40<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data finished! \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%run preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63b413d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8250ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aad63f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d531dd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_dir='metadata'\n",
    "processed_dir='processed data'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a32287de",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_dir=Path(metadata_dir)\n",
    "processed_dir=Path(processed_dir)\n",
    "traincsv_dir=metadata_dir/\"train_set.csv\"\n",
    "train_meta=pd.read_csv(traincsv_dir)\n",
    "meta_csvdir=metadata_dir/'metadata.csv'\n",
    "metadata=pd.read_csv(meta_csvdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5f1c28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N185326362.pt\n"
     ]
    }
   ],
   "source": [
    "patient_pat=metadata.loc[0][\"filepath\"].split('\\\\')[-1]\n",
    "print(patient_pat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c808b2",
   "metadata": {},
   "source": [
    "count how many normal/ abnormal in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe6f846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_normal=[]\n",
    "total_abnormal=[]\n",
    "for i in range(len(metadata[\"filepath\"])):\n",
    "    patient_pat=metadata.loc[i][\"filepath\"].split('\\\\')\n",
    "    patient=patient_pat[-1]\n",
    "    patient='processed data\\\\'+patient\n",
    "\n",
    "    data=torch.load(patient)\n",
    "\n",
    "    if data[\"label\"]==1:\n",
    "        total_abnormal.append(patient)\n",
    "    else:\n",
    "        total_normal.append(patient)\n",
    "     \n",
    "        \n",
    "    \n",
    "\n",
    "                                                    \n",
    "                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7899c90",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total_normal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mtotal_normal\u001b[49m))\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(total_abnormal))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'total_normal' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(total_normal))\n",
    "print(len(total_abnormal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75d80f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9\n"
     ]
    }
   ],
   "source": [
    "print(len(total_normal)/len(total_abnormal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5142eba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run augmentation.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29629332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n",
      "114\n"
     ]
    }
   ],
   "source": [
    "train_meta=pd.read_csv(traincsv_dir)\n",
    "\n",
    "train_normal=[]\n",
    "train_abnormal=[]\n",
    "\n",
    "for i in range(len(train_meta[\"filepath\"])):\n",
    "    patient_pat=train_meta.loc[i][\"filepath\"].split('\\\\')\n",
    "    patient=patient_pat[-1]\n",
    "    data=torch.load('processed data\\\\'+patient)\n",
    "\n",
    "    if data[\"label\"]==1:\n",
    "        train_abnormal.append(patient)\n",
    "    else:\n",
    "        train_normal.append(patient)\n",
    "\n",
    "print(len(train_normal))\n",
    "print(len(train_abnormal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91234eca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "984a160f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-5729fc018a9529dc\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-5729fc018a9529dc\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6555bab5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "# %reload_ext tensorboard\n",
    "%tensorboard --logdir=runs --host localhost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f960f752",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2877679443.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[28], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    del /q %TMP%\\.tensorboard-info\\*\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "!taskkill /im tensorboard.exe /f\n",
    "del /q %TMP%\\.tensorboard-info\\*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e0a916c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'checkpoint_dir': 'checkpoints/checkpoints_FFC_res18', 'checkpoint_name': 'model.ckpt-10000.pt', 'dataset_dir': '../../../metadata', 'type': 'FFC_res18'}\n",
      "..\\..\\..\\metadata\\test_set.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 10.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPI test:\n",
      "{'average_acc': 0.8461538461538461, 'average_precision': 0.4993894993894994, 'recall': 0.5555555555555556, 'auc': 0.7444444444444445, 'confusion_matrix': array([[28,  2],\n",
      "       [ 4,  5]], dtype=int64)}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.90        30\n",
      "           1       0.71      0.56      0.63         9\n",
      "\n",
      "    accuracy                           0.85        39\n",
      "   macro avg       0.79      0.74      0.76        39\n",
      "weighted avg       0.84      0.85      0.84        39\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run test_single.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa44e3eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83a49dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import MPIDataset,save_checkpoint, load_checkpoint,Evaluator\n",
    "from models.Classifiers import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4386975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=FFC_res18()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "checkpoint_dir=Path(r\"checkpoints/checkpoints_FFC_res18\")/\"model.ckpt-10000.pt\"\n",
    "checkpoint=torch.load(checkpoint_dir)\n",
    "model.load_state_dict(checkpoint[\"model\"])  \n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "MPI_testset=MPIDataset(Path(\"metadata\")/'test_cam.csv')\n",
    "MPI_dataloader=DataLoader(\n",
    "    MPI_testset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c0823ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "tensor([[0.0206]], device='cuda:0')\n",
      "8\n",
      "tensor([[0.9551]], device='cuda:0')\n",
      "9\n",
      "tensor([[2.3226e-07]], device='cuda:0')\n",
      "10\n",
      "tensor([[0.8819]], device='cuda:0')\n",
      "20\n",
      "tensor([[0.0171]], device='cuda:0')\n",
      "23\n",
      "tensor([[0.9997]], device='cuda:0')\n",
      "24\n",
      "tensor([[0.9999]], device='cuda:0')\n",
      "28\n",
      "tensor([[0.9998]], device='cuda:0')\n",
      "30\n",
      "tensor([[1.3561e-05]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataiter=iter(MPI_dataloader)\n",
    "data=next(dataiter)\n",
    "\n",
    "for i in range(len(MPI_testset)):\n",
    "    if torch.eq(data[\"label\"][i].to(device),1):\n",
    "        with torch.no_grad():\n",
    "            mpi=data[\"image\"][i].unsqueeze(0).to(device)\n",
    "            print(i)\n",
    "            out=torch.nn.functional.sigmoid(model(mpi))\n",
    "            print(out)\n",
    "\n",
    "    else : continue\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10c2e561",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
    "from pytorch_grad_cam.utils.model_targets import BinaryClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "import cv2\n",
    "class GradCAM_(GradCAM):\n",
    "    def __init__(self, model, target_layers, use_cuda=False,\n",
    "                 reshape_transform=None):\n",
    "        super(\n",
    "            GradCAM,\n",
    "            self).__init__(\n",
    "            model,\n",
    "            target_layers,\n",
    "            use_cuda,\n",
    "            reshape_transform)\n",
    "\n",
    "    def get_cam_weights(self,\n",
    "                        input_tensor,\n",
    "                        target_layer,\n",
    "                        target_category,\n",
    "                        activations,\n",
    "                        grads):\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86fc9d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1.5784]],\n",
      "\n",
      "         [[1.8085]],\n",
      "\n",
      "         [[0.3257]],\n",
      "\n",
      "         [[0.1097]],\n",
      "\n",
      "         [[0.1931]],\n",
      "\n",
      "         [[1.4597]],\n",
      "\n",
      "         [[0.2395]],\n",
      "\n",
      "         [[0.2283]],\n",
      "\n",
      "         [[0.1248]],\n",
      "\n",
      "         [[0.1422]],\n",
      "\n",
      "         [[0.1604]],\n",
      "\n",
      "         [[0.1862]],\n",
      "\n",
      "         [[0.1534]],\n",
      "\n",
      "         [[0.3542]],\n",
      "\n",
      "         [[0.1752]],\n",
      "\n",
      "         [[2.9027]],\n",
      "\n",
      "         [[2.6339]],\n",
      "\n",
      "         [[2.7813]],\n",
      "\n",
      "         [[2.6704]],\n",
      "\n",
      "         [[0.2029]],\n",
      "\n",
      "         [[1.8808]],\n",
      "\n",
      "         [[0.1021]],\n",
      "\n",
      "         [[2.4967]],\n",
      "\n",
      "         [[0.1142]],\n",
      "\n",
      "         [[0.3850]],\n",
      "\n",
      "         [[2.5425]],\n",
      "\n",
      "         [[1.6166]],\n",
      "\n",
      "         [[0.2039]],\n",
      "\n",
      "         [[0.2503]],\n",
      "\n",
      "         [[3.0614]],\n",
      "\n",
      "         [[3.0700]],\n",
      "\n",
      "         [[0.3760]],\n",
      "\n",
      "         [[0.1424]],\n",
      "\n",
      "         [[2.9450]],\n",
      "\n",
      "         [[2.2758]],\n",
      "\n",
      "         [[0.2043]],\n",
      "\n",
      "         [[0.3479]],\n",
      "\n",
      "         [[0.1179]],\n",
      "\n",
      "         [[0.2139]],\n",
      "\n",
      "         [[0.2321]],\n",
      "\n",
      "         [[0.1070]],\n",
      "\n",
      "         [[0.2282]],\n",
      "\n",
      "         [[2.7164]],\n",
      "\n",
      "         [[2.9349]],\n",
      "\n",
      "         [[2.9465]],\n",
      "\n",
      "         [[2.4379]],\n",
      "\n",
      "         [[0.2639]],\n",
      "\n",
      "         [[0.1443]],\n",
      "\n",
      "         [[1.8741]],\n",
      "\n",
      "         [[3.1092]],\n",
      "\n",
      "         [[3.0496]],\n",
      "\n",
      "         [[2.1545]],\n",
      "\n",
      "         [[1.8996]],\n",
      "\n",
      "         [[0.2692]],\n",
      "\n",
      "         [[0.1223]],\n",
      "\n",
      "         [[1.1773]],\n",
      "\n",
      "         [[0.1689]],\n",
      "\n",
      "         [[2.9389]],\n",
      "\n",
      "         [[0.1200]],\n",
      "\n",
      "         [[0.4554]],\n",
      "\n",
      "         [[3.1798]],\n",
      "\n",
      "         [[0.1492]],\n",
      "\n",
      "         [[0.1504]],\n",
      "\n",
      "         [[2.8691]],\n",
      "\n",
      "         [[0.3095]],\n",
      "\n",
      "         [[0.1688]],\n",
      "\n",
      "         [[0.1694]],\n",
      "\n",
      "         [[0.1178]],\n",
      "\n",
      "         [[2.9125]],\n",
      "\n",
      "         [[1.6625]],\n",
      "\n",
      "         [[0.1833]],\n",
      "\n",
      "         [[3.0816]],\n",
      "\n",
      "         [[2.7815]],\n",
      "\n",
      "         [[2.8858]],\n",
      "\n",
      "         [[0.1199]],\n",
      "\n",
      "         [[2.5203]],\n",
      "\n",
      "         [[0.2361]],\n",
      "\n",
      "         [[0.5314]],\n",
      "\n",
      "         [[3.0737]],\n",
      "\n",
      "         [[0.1551]],\n",
      "\n",
      "         [[0.6918]],\n",
      "\n",
      "         [[2.8752]],\n",
      "\n",
      "         [[0.4862]],\n",
      "\n",
      "         [[0.1018]],\n",
      "\n",
      "         [[1.8113]],\n",
      "\n",
      "         [[0.0892]],\n",
      "\n",
      "         [[0.2522]],\n",
      "\n",
      "         [[0.0871]],\n",
      "\n",
      "         [[0.1186]],\n",
      "\n",
      "         [[2.6977]],\n",
      "\n",
      "         [[2.9469]],\n",
      "\n",
      "         [[0.2202]],\n",
      "\n",
      "         [[3.0760]],\n",
      "\n",
      "         [[1.2490]],\n",
      "\n",
      "         [[3.0423]],\n",
      "\n",
      "         [[0.1898]],\n",
      "\n",
      "         [[1.6568]],\n",
      "\n",
      "         [[0.0406]],\n",
      "\n",
      "         [[2.7499]],\n",
      "\n",
      "         [[0.3556]],\n",
      "\n",
      "         [[3.0630]],\n",
      "\n",
      "         [[3.2728]],\n",
      "\n",
      "         [[0.7082]],\n",
      "\n",
      "         [[2.8405]],\n",
      "\n",
      "         [[0.1457]],\n",
      "\n",
      "         [[0.4732]],\n",
      "\n",
      "         [[2.1573]],\n",
      "\n",
      "         [[2.1122]],\n",
      "\n",
      "         [[0.3252]],\n",
      "\n",
      "         [[0.1858]],\n",
      "\n",
      "         [[0.1976]],\n",
      "\n",
      "         [[0.0857]],\n",
      "\n",
      "         [[1.1673]],\n",
      "\n",
      "         [[0.1930]],\n",
      "\n",
      "         [[0.1917]],\n",
      "\n",
      "         [[0.1221]],\n",
      "\n",
      "         [[1.2729]],\n",
      "\n",
      "         [[1.7625]],\n",
      "\n",
      "         [[0.3151]],\n",
      "\n",
      "         [[0.1859]],\n",
      "\n",
      "         [[0.2170]],\n",
      "\n",
      "         [[3.0238]],\n",
      "\n",
      "         [[0.1583]],\n",
      "\n",
      "         [[2.6619]],\n",
      "\n",
      "         [[0.4975]],\n",
      "\n",
      "         [[2.7742]],\n",
      "\n",
      "         [[2.4045]],\n",
      "\n",
      "         [[0.2282]],\n",
      "\n",
      "         [[0.3055]],\n",
      "\n",
      "         [[0.1863]],\n",
      "\n",
      "         [[0.4445]],\n",
      "\n",
      "         [[1.3447]],\n",
      "\n",
      "         [[3.0803]],\n",
      "\n",
      "         [[2.6127]],\n",
      "\n",
      "         [[0.3276]],\n",
      "\n",
      "         [[0.2634]],\n",
      "\n",
      "         [[0.3115]],\n",
      "\n",
      "         [[1.0180]],\n",
      "\n",
      "         [[0.0954]],\n",
      "\n",
      "         [[0.3280]],\n",
      "\n",
      "         [[0.4508]],\n",
      "\n",
      "         [[0.3634]],\n",
      "\n",
      "         [[0.1691]],\n",
      "\n",
      "         [[1.4515]],\n",
      "\n",
      "         [[1.3527]],\n",
      "\n",
      "         [[0.0800]],\n",
      "\n",
      "         [[0.6481]],\n",
      "\n",
      "         [[0.1247]],\n",
      "\n",
      "         [[2.7613]],\n",
      "\n",
      "         [[0.5683]],\n",
      "\n",
      "         [[3.2674]],\n",
      "\n",
      "         [[0.2185]],\n",
      "\n",
      "         [[3.1502]],\n",
      "\n",
      "         [[2.5440]],\n",
      "\n",
      "         [[0.3508]],\n",
      "\n",
      "         [[0.3187]],\n",
      "\n",
      "         [[0.4334]],\n",
      "\n",
      "         [[0.1367]],\n",
      "\n",
      "         [[2.4522]],\n",
      "\n",
      "         [[0.1525]],\n",
      "\n",
      "         [[0.3358]],\n",
      "\n",
      "         [[0.0817]],\n",
      "\n",
      "         [[3.1967]],\n",
      "\n",
      "         [[0.3183]],\n",
      "\n",
      "         [[2.9865]],\n",
      "\n",
      "         [[2.2251]],\n",
      "\n",
      "         [[0.2311]],\n",
      "\n",
      "         [[3.1225]],\n",
      "\n",
      "         [[0.1414]],\n",
      "\n",
      "         [[0.1405]],\n",
      "\n",
      "         [[0.1339]],\n",
      "\n",
      "         [[0.1080]],\n",
      "\n",
      "         [[1.3979]],\n",
      "\n",
      "         [[0.1845]],\n",
      "\n",
      "         [[1.3565]],\n",
      "\n",
      "         [[1.0592]],\n",
      "\n",
      "         [[0.3598]],\n",
      "\n",
      "         [[0.3179]],\n",
      "\n",
      "         [[0.3386]],\n",
      "\n",
      "         [[3.1869]],\n",
      "\n",
      "         [[0.1327]],\n",
      "\n",
      "         [[0.1403]],\n",
      "\n",
      "         [[1.2477]],\n",
      "\n",
      "         [[0.1618]],\n",
      "\n",
      "         [[1.1901]],\n",
      "\n",
      "         [[1.3870]],\n",
      "\n",
      "         [[0.2055]],\n",
      "\n",
      "         [[0.2546]],\n",
      "\n",
      "         [[1.1929]],\n",
      "\n",
      "         [[3.0840]],\n",
      "\n",
      "         [[2.7173]],\n",
      "\n",
      "         [[0.1534]],\n",
      "\n",
      "         [[2.8499]],\n",
      "\n",
      "         [[2.9788]],\n",
      "\n",
      "         [[1.8772]],\n",
      "\n",
      "         [[0.3360]],\n",
      "\n",
      "         [[0.3430]],\n",
      "\n",
      "         [[0.9113]],\n",
      "\n",
      "         [[0.1325]],\n",
      "\n",
      "         [[2.3205]],\n",
      "\n",
      "         [[0.1188]],\n",
      "\n",
      "         [[1.2138]],\n",
      "\n",
      "         [[0.2076]],\n",
      "\n",
      "         [[2.4489]],\n",
      "\n",
      "         [[0.7946]],\n",
      "\n",
      "         [[0.1669]],\n",
      "\n",
      "         [[2.8784]],\n",
      "\n",
      "         [[0.8364]],\n",
      "\n",
      "         [[2.7361]],\n",
      "\n",
      "         [[0.3750]],\n",
      "\n",
      "         [[3.0244]],\n",
      "\n",
      "         [[0.4154]],\n",
      "\n",
      "         [[0.2126]],\n",
      "\n",
      "         [[0.2413]],\n",
      "\n",
      "         [[1.9424]],\n",
      "\n",
      "         [[0.2413]],\n",
      "\n",
      "         [[0.3188]],\n",
      "\n",
      "         [[2.8961]],\n",
      "\n",
      "         [[0.4717]],\n",
      "\n",
      "         [[0.6192]],\n",
      "\n",
      "         [[0.1307]],\n",
      "\n",
      "         [[2.6199]],\n",
      "\n",
      "         [[0.4495]],\n",
      "\n",
      "         [[2.6945]],\n",
      "\n",
      "         [[0.2135]],\n",
      "\n",
      "         [[0.3226]],\n",
      "\n",
      "         [[2.5361]],\n",
      "\n",
      "         [[0.0426]],\n",
      "\n",
      "         [[0.1475]],\n",
      "\n",
      "         [[0.2559]],\n",
      "\n",
      "         [[0.0257]],\n",
      "\n",
      "         [[2.8267]],\n",
      "\n",
      "         [[0.1620]],\n",
      "\n",
      "         [[0.1539]],\n",
      "\n",
      "         [[0.0333]],\n",
      "\n",
      "         [[0.0718]],\n",
      "\n",
      "         [[0.6059]],\n",
      "\n",
      "         [[0.3785]],\n",
      "\n",
      "         [[0.2842]],\n",
      "\n",
      "         [[2.2292]],\n",
      "\n",
      "         [[0.1308]],\n",
      "\n",
      "         [[0.0764]],\n",
      "\n",
      "         [[0.0838]],\n",
      "\n",
      "         [[0.2697]],\n",
      "\n",
      "         [[0.4902]],\n",
      "\n",
      "         [[0.1546]],\n",
      "\n",
      "         [[0.9783]],\n",
      "\n",
      "         [[0.2661]],\n",
      "\n",
      "         [[0.1998]],\n",
      "\n",
      "         [[0.2383]],\n",
      "\n",
      "         [[0.2479]],\n",
      "\n",
      "         [[0.1129]],\n",
      "\n",
      "         [[1.9494]],\n",
      "\n",
      "         [[1.2985]],\n",
      "\n",
      "         [[0.3007]],\n",
      "\n",
      "         [[2.8475]],\n",
      "\n",
      "         [[0.5777]],\n",
      "\n",
      "         [[0.2010]],\n",
      "\n",
      "         [[0.6114]],\n",
      "\n",
      "         [[0.8730]],\n",
      "\n",
      "         [[0.0577]],\n",
      "\n",
      "         [[2.6785]],\n",
      "\n",
      "         [[3.1324]],\n",
      "\n",
      "         [[0.1400]],\n",
      "\n",
      "         [[2.9146]],\n",
      "\n",
      "         [[3.2456]],\n",
      "\n",
      "         [[2.9944]],\n",
      "\n",
      "         [[0.1699]],\n",
      "\n",
      "         [[0.1643]],\n",
      "\n",
      "         [[0.0718]],\n",
      "\n",
      "         [[0.4031]],\n",
      "\n",
      "         [[1.2194]],\n",
      "\n",
      "         [[0.1093]],\n",
      "\n",
      "         [[0.1015]],\n",
      "\n",
      "         [[1.6197]],\n",
      "\n",
      "         [[3.1852]],\n",
      "\n",
      "         [[0.2539]],\n",
      "\n",
      "         [[2.9519]],\n",
      "\n",
      "         [[1.8116]],\n",
      "\n",
      "         [[0.1853]],\n",
      "\n",
      "         [[3.1380]],\n",
      "\n",
      "         [[0.2623]],\n",
      "\n",
      "         [[0.2089]],\n",
      "\n",
      "         [[0.3245]],\n",
      "\n",
      "         [[0.1092]],\n",
      "\n",
      "         [[2.8761]],\n",
      "\n",
      "         [[0.0297]],\n",
      "\n",
      "         [[2.3523]],\n",
      "\n",
      "         [[2.9740]],\n",
      "\n",
      "         [[1.2585]],\n",
      "\n",
      "         [[2.6449]],\n",
      "\n",
      "         [[0.0593]],\n",
      "\n",
      "         [[0.4534]],\n",
      "\n",
      "         [[0.1434]],\n",
      "\n",
      "         [[0.2023]],\n",
      "\n",
      "         [[1.6451]],\n",
      "\n",
      "         [[1.2657]],\n",
      "\n",
      "         [[2.7197]],\n",
      "\n",
      "         [[2.7539]],\n",
      "\n",
      "         [[2.3179]],\n",
      "\n",
      "         [[2.9051]],\n",
      "\n",
      "         [[0.1607]],\n",
      "\n",
      "         [[1.3182]],\n",
      "\n",
      "         [[3.1827]],\n",
      "\n",
      "         [[0.1013]],\n",
      "\n",
      "         [[0.2062]],\n",
      "\n",
      "         [[0.1325]],\n",
      "\n",
      "         [[0.0579]],\n",
      "\n",
      "         [[0.2779]],\n",
      "\n",
      "         [[2.8305]],\n",
      "\n",
      "         [[0.4055]],\n",
      "\n",
      "         [[0.2010]],\n",
      "\n",
      "         [[0.4500]],\n",
      "\n",
      "         [[1.0106]],\n",
      "\n",
      "         [[0.3143]],\n",
      "\n",
      "         [[2.7535]],\n",
      "\n",
      "         [[0.1352]],\n",
      "\n",
      "         [[0.2032]],\n",
      "\n",
      "         [[2.1164]],\n",
      "\n",
      "         [[0.6074]],\n",
      "\n",
      "         [[0.1854]],\n",
      "\n",
      "         [[0.1489]],\n",
      "\n",
      "         [[0.1269]],\n",
      "\n",
      "         [[0.1318]],\n",
      "\n",
      "         [[0.4362]],\n",
      "\n",
      "         [[2.9645]],\n",
      "\n",
      "         [[2.5869]],\n",
      "\n",
      "         [[0.0962]],\n",
      "\n",
      "         [[0.1815]],\n",
      "\n",
      "         [[1.4027]],\n",
      "\n",
      "         [[3.0700]],\n",
      "\n",
      "         [[0.0857]],\n",
      "\n",
      "         [[0.2116]],\n",
      "\n",
      "         [[0.4663]],\n",
      "\n",
      "         [[0.1554]],\n",
      "\n",
      "         [[2.7355]],\n",
      "\n",
      "         [[0.1516]],\n",
      "\n",
      "         [[0.2351]],\n",
      "\n",
      "         [[2.5931]],\n",
      "\n",
      "         [[1.7026]],\n",
      "\n",
      "         [[0.3800]],\n",
      "\n",
      "         [[2.7718]],\n",
      "\n",
      "         [[1.5151]],\n",
      "\n",
      "         [[0.2570]],\n",
      "\n",
      "         [[0.0772]],\n",
      "\n",
      "         [[0.5174]],\n",
      "\n",
      "         [[0.4512]],\n",
      "\n",
      "         [[0.5822]],\n",
      "\n",
      "         [[0.2476]],\n",
      "\n",
      "         [[3.0425]],\n",
      "\n",
      "         [[0.1710]],\n",
      "\n",
      "         [[0.5004]],\n",
      "\n",
      "         [[0.2342]],\n",
      "\n",
      "         [[2.8457]],\n",
      "\n",
      "         [[2.9485]],\n",
      "\n",
      "         [[0.2340]],\n",
      "\n",
      "         [[0.3189]],\n",
      "\n",
      "         [[2.6099]],\n",
      "\n",
      "         [[0.0416]],\n",
      "\n",
      "         [[0.1794]],\n",
      "\n",
      "         [[0.0880]],\n",
      "\n",
      "         [[1.9738]],\n",
      "\n",
      "         [[1.4902]],\n",
      "\n",
      "         [[0.4846]],\n",
      "\n",
      "         [[0.1855]],\n",
      "\n",
      "         [[0.5043]],\n",
      "\n",
      "         [[0.5405]],\n",
      "\n",
      "         [[3.0278]],\n",
      "\n",
      "         [[0.1453]],\n",
      "\n",
      "         [[0.1095]],\n",
      "\n",
      "         [[2.8529]],\n",
      "\n",
      "         [[3.0682]],\n",
      "\n",
      "         [[2.8265]],\n",
      "\n",
      "         [[0.0946]],\n",
      "\n",
      "         [[3.2208]],\n",
      "\n",
      "         [[0.5846]],\n",
      "\n",
      "         [[0.2840]],\n",
      "\n",
      "         [[1.2938]],\n",
      "\n",
      "         [[0.6300]],\n",
      "\n",
      "         [[2.5922]],\n",
      "\n",
      "         [[3.1850]],\n",
      "\n",
      "         [[1.9445]],\n",
      "\n",
      "         [[1.5217]],\n",
      "\n",
      "         [[0.0282]],\n",
      "\n",
      "         [[0.5756]],\n",
      "\n",
      "         [[2.0254]],\n",
      "\n",
      "         [[0.2331]],\n",
      "\n",
      "         [[2.7223]],\n",
      "\n",
      "         [[0.2998]],\n",
      "\n",
      "         [[2.0225]],\n",
      "\n",
      "         [[0.2069]],\n",
      "\n",
      "         [[0.1248]],\n",
      "\n",
      "         [[0.0495]],\n",
      "\n",
      "         [[2.6829]],\n",
      "\n",
      "         [[2.3955]],\n",
      "\n",
      "         [[0.4339]],\n",
      "\n",
      "         [[1.4189]],\n",
      "\n",
      "         [[2.6408]],\n",
      "\n",
      "         [[0.6525]],\n",
      "\n",
      "         [[0.1101]],\n",
      "\n",
      "         [[0.3400]],\n",
      "\n",
      "         [[0.0758]],\n",
      "\n",
      "         [[3.3618]],\n",
      "\n",
      "         [[0.2303]],\n",
      "\n",
      "         [[1.7265]],\n",
      "\n",
      "         [[0.2908]],\n",
      "\n",
      "         [[0.7883]],\n",
      "\n",
      "         [[0.2281]],\n",
      "\n",
      "         [[2.8828]],\n",
      "\n",
      "         [[0.5261]],\n",
      "\n",
      "         [[2.8860]],\n",
      "\n",
      "         [[3.1045]],\n",
      "\n",
      "         [[0.2200]],\n",
      "\n",
      "         [[1.2130]],\n",
      "\n",
      "         [[1.4881]],\n",
      "\n",
      "         [[0.1928]],\n",
      "\n",
      "         [[0.0660]],\n",
      "\n",
      "         [[0.0967]],\n",
      "\n",
      "         [[2.7290]],\n",
      "\n",
      "         [[0.1960]],\n",
      "\n",
      "         [[0.3217]],\n",
      "\n",
      "         [[0.2220]],\n",
      "\n",
      "         [[2.8740]],\n",
      "\n",
      "         [[1.3047]],\n",
      "\n",
      "         [[3.0873]],\n",
      "\n",
      "         [[0.1377]],\n",
      "\n",
      "         [[0.2750]],\n",
      "\n",
      "         [[2.6149]],\n",
      "\n",
      "         [[0.1417]],\n",
      "\n",
      "         [[0.1763]],\n",
      "\n",
      "         [[0.0914]],\n",
      "\n",
      "         [[2.6074]],\n",
      "\n",
      "         [[0.2552]],\n",
      "\n",
      "         [[0.0912]],\n",
      "\n",
      "         [[1.4314]],\n",
      "\n",
      "         [[0.2283]],\n",
      "\n",
      "         [[1.4544]],\n",
      "\n",
      "         [[0.1215]],\n",
      "\n",
      "         [[2.3567]],\n",
      "\n",
      "         [[0.1774]],\n",
      "\n",
      "         [[0.3957]],\n",
      "\n",
      "         [[0.5340]],\n",
      "\n",
      "         [[2.8732]],\n",
      "\n",
      "         [[0.0975]],\n",
      "\n",
      "         [[0.3948]],\n",
      "\n",
      "         [[2.9393]],\n",
      "\n",
      "         [[0.1197]],\n",
      "\n",
      "         [[0.7456]],\n",
      "\n",
      "         [[0.1528]],\n",
      "\n",
      "         [[0.2819]],\n",
      "\n",
      "         [[0.2539]],\n",
      "\n",
      "         [[0.2686]],\n",
      "\n",
      "         [[0.3766]],\n",
      "\n",
      "         [[0.2312]],\n",
      "\n",
      "         [[0.2014]],\n",
      "\n",
      "         [[0.0829]],\n",
      "\n",
      "         [[2.2967]],\n",
      "\n",
      "         [[3.0183]],\n",
      "\n",
      "         [[0.6363]],\n",
      "\n",
      "         [[3.0987]],\n",
      "\n",
      "         [[2.7683]],\n",
      "\n",
      "         [[2.4913]],\n",
      "\n",
      "         [[0.3129]],\n",
      "\n",
      "         [[0.3943]],\n",
      "\n",
      "         [[2.6180]],\n",
      "\n",
      "         [[0.3854]],\n",
      "\n",
      "         [[0.4154]],\n",
      "\n",
      "         [[0.1318]],\n",
      "\n",
      "         [[0.0985]],\n",
      "\n",
      "         [[2.8822]],\n",
      "\n",
      "         [[0.1903]],\n",
      "\n",
      "         [[0.3342]],\n",
      "\n",
      "         [[0.3060]],\n",
      "\n",
      "         [[0.3976]],\n",
      "\n",
      "         [[1.3235]],\n",
      "\n",
      "         [[0.1987]],\n",
      "\n",
      "         [[0.1275]],\n",
      "\n",
      "         [[0.3229]],\n",
      "\n",
      "         [[2.9467]],\n",
      "\n",
      "         [[2.5833]],\n",
      "\n",
      "         [[0.4956]],\n",
      "\n",
      "         [[0.1759]],\n",
      "\n",
      "         [[2.0665]],\n",
      "\n",
      "         [[2.8104]],\n",
      "\n",
      "         [[0.3382]],\n",
      "\n",
      "         [[1.1912]],\n",
      "\n",
      "         [[0.1937]],\n",
      "\n",
      "         [[2.7672]],\n",
      "\n",
      "         [[0.2671]],\n",
      "\n",
      "         [[0.3595]],\n",
      "\n",
      "         [[0.1816]],\n",
      "\n",
      "         [[2.4862]],\n",
      "\n",
      "         [[0.3299]],\n",
      "\n",
      "         [[1.2413]],\n",
      "\n",
      "         [[3.1024]],\n",
      "\n",
      "         [[0.2254]],\n",
      "\n",
      "         [[1.4926]],\n",
      "\n",
      "         [[2.5833]],\n",
      "\n",
      "         [[0.2794]],\n",
      "\n",
      "         [[0.2130]],\n",
      "\n",
      "         [[2.9302]],\n",
      "\n",
      "         [[2.7186]],\n",
      "\n",
      "         [[0.1122]],\n",
      "\n",
      "         [[0.1057]],\n",
      "\n",
      "         [[3.2715]],\n",
      "\n",
      "         [[1.7156]],\n",
      "\n",
      "         [[0.1810]],\n",
      "\n",
      "         [[0.0842]],\n",
      "\n",
      "         [[1.2549]],\n",
      "\n",
      "         [[0.4316]],\n",
      "\n",
      "         [[0.1951]],\n",
      "\n",
      "         [[0.4366]]]], device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor([[[[1.5784]],\n",
      "\n",
      "         [[1.8085]],\n",
      "\n",
      "         [[0.3257]],\n",
      "\n",
      "         [[0.1097]],\n",
      "\n",
      "         [[0.1931]],\n",
      "\n",
      "         [[1.4597]],\n",
      "\n",
      "         [[0.2395]],\n",
      "\n",
      "         [[0.2283]],\n",
      "\n",
      "         [[0.1248]],\n",
      "\n",
      "         [[0.1422]],\n",
      "\n",
      "         [[0.1604]],\n",
      "\n",
      "         [[0.1862]],\n",
      "\n",
      "         [[0.1534]],\n",
      "\n",
      "         [[0.3542]],\n",
      "\n",
      "         [[0.1752]],\n",
      "\n",
      "         [[2.9027]],\n",
      "\n",
      "         [[2.6339]],\n",
      "\n",
      "         [[2.7813]],\n",
      "\n",
      "         [[2.6704]],\n",
      "\n",
      "         [[0.2029]],\n",
      "\n",
      "         [[1.8808]],\n",
      "\n",
      "         [[0.1021]],\n",
      "\n",
      "         [[2.4967]],\n",
      "\n",
      "         [[0.1142]],\n",
      "\n",
      "         [[0.3850]],\n",
      "\n",
      "         [[2.5425]],\n",
      "\n",
      "         [[1.6166]],\n",
      "\n",
      "         [[0.2039]],\n",
      "\n",
      "         [[0.2503]],\n",
      "\n",
      "         [[3.0614]],\n",
      "\n",
      "         [[3.0700]],\n",
      "\n",
      "         [[0.3760]],\n",
      "\n",
      "         [[0.1424]],\n",
      "\n",
      "         [[2.9450]],\n",
      "\n",
      "         [[2.2758]],\n",
      "\n",
      "         [[0.2043]],\n",
      "\n",
      "         [[0.3479]],\n",
      "\n",
      "         [[0.1179]],\n",
      "\n",
      "         [[0.2139]],\n",
      "\n",
      "         [[0.2321]],\n",
      "\n",
      "         [[0.1070]],\n",
      "\n",
      "         [[0.2282]],\n",
      "\n",
      "         [[2.7164]],\n",
      "\n",
      "         [[2.9349]],\n",
      "\n",
      "         [[2.9465]],\n",
      "\n",
      "         [[2.4379]],\n",
      "\n",
      "         [[0.2639]],\n",
      "\n",
      "         [[0.1443]],\n",
      "\n",
      "         [[1.8741]],\n",
      "\n",
      "         [[3.1092]],\n",
      "\n",
      "         [[3.0496]],\n",
      "\n",
      "         [[2.1545]],\n",
      "\n",
      "         [[1.8996]],\n",
      "\n",
      "         [[0.2692]],\n",
      "\n",
      "         [[0.1223]],\n",
      "\n",
      "         [[1.1773]],\n",
      "\n",
      "         [[0.1689]],\n",
      "\n",
      "         [[2.9389]],\n",
      "\n",
      "         [[0.1200]],\n",
      "\n",
      "         [[0.4554]],\n",
      "\n",
      "         [[3.1798]],\n",
      "\n",
      "         [[0.1492]],\n",
      "\n",
      "         [[0.1504]],\n",
      "\n",
      "         [[2.8691]],\n",
      "\n",
      "         [[0.3095]],\n",
      "\n",
      "         [[0.1688]],\n",
      "\n",
      "         [[0.1694]],\n",
      "\n",
      "         [[0.1178]],\n",
      "\n",
      "         [[2.9125]],\n",
      "\n",
      "         [[1.6625]],\n",
      "\n",
      "         [[0.1833]],\n",
      "\n",
      "         [[3.0816]],\n",
      "\n",
      "         [[2.7815]],\n",
      "\n",
      "         [[2.8858]],\n",
      "\n",
      "         [[0.1199]],\n",
      "\n",
      "         [[2.5203]],\n",
      "\n",
      "         [[0.2361]],\n",
      "\n",
      "         [[0.5314]],\n",
      "\n",
      "         [[3.0737]],\n",
      "\n",
      "         [[0.1551]],\n",
      "\n",
      "         [[0.6918]],\n",
      "\n",
      "         [[2.8752]],\n",
      "\n",
      "         [[0.4862]],\n",
      "\n",
      "         [[0.1018]],\n",
      "\n",
      "         [[1.8113]],\n",
      "\n",
      "         [[0.0892]],\n",
      "\n",
      "         [[0.2522]],\n",
      "\n",
      "         [[0.0871]],\n",
      "\n",
      "         [[0.1186]],\n",
      "\n",
      "         [[2.6977]],\n",
      "\n",
      "         [[2.9469]],\n",
      "\n",
      "         [[0.2202]],\n",
      "\n",
      "         [[3.0760]],\n",
      "\n",
      "         [[1.2490]],\n",
      "\n",
      "         [[3.0423]],\n",
      "\n",
      "         [[0.1898]],\n",
      "\n",
      "         [[1.6568]],\n",
      "\n",
      "         [[0.0406]],\n",
      "\n",
      "         [[2.7499]],\n",
      "\n",
      "         [[0.3556]],\n",
      "\n",
      "         [[3.0630]],\n",
      "\n",
      "         [[3.2728]],\n",
      "\n",
      "         [[0.7082]],\n",
      "\n",
      "         [[2.8405]],\n",
      "\n",
      "         [[0.1457]],\n",
      "\n",
      "         [[0.4732]],\n",
      "\n",
      "         [[2.1573]],\n",
      "\n",
      "         [[2.1122]],\n",
      "\n",
      "         [[0.3252]],\n",
      "\n",
      "         [[0.1858]],\n",
      "\n",
      "         [[0.1976]],\n",
      "\n",
      "         [[0.0857]],\n",
      "\n",
      "         [[1.1673]],\n",
      "\n",
      "         [[0.1930]],\n",
      "\n",
      "         [[0.1917]],\n",
      "\n",
      "         [[0.1221]],\n",
      "\n",
      "         [[1.2729]],\n",
      "\n",
      "         [[1.7625]],\n",
      "\n",
      "         [[0.3151]],\n",
      "\n",
      "         [[0.1859]],\n",
      "\n",
      "         [[0.2170]],\n",
      "\n",
      "         [[3.0238]],\n",
      "\n",
      "         [[0.1583]],\n",
      "\n",
      "         [[2.6619]],\n",
      "\n",
      "         [[0.4975]],\n",
      "\n",
      "         [[2.7742]],\n",
      "\n",
      "         [[2.4045]],\n",
      "\n",
      "         [[0.2282]],\n",
      "\n",
      "         [[0.3055]],\n",
      "\n",
      "         [[0.1863]],\n",
      "\n",
      "         [[0.4445]],\n",
      "\n",
      "         [[1.3447]],\n",
      "\n",
      "         [[3.0803]],\n",
      "\n",
      "         [[2.6127]],\n",
      "\n",
      "         [[0.3276]],\n",
      "\n",
      "         [[0.2634]],\n",
      "\n",
      "         [[0.3115]],\n",
      "\n",
      "         [[1.0180]],\n",
      "\n",
      "         [[0.0954]],\n",
      "\n",
      "         [[0.3280]],\n",
      "\n",
      "         [[0.4508]],\n",
      "\n",
      "         [[0.3634]],\n",
      "\n",
      "         [[0.1691]],\n",
      "\n",
      "         [[1.4515]],\n",
      "\n",
      "         [[1.3527]],\n",
      "\n",
      "         [[0.0800]],\n",
      "\n",
      "         [[0.6481]],\n",
      "\n",
      "         [[0.1247]],\n",
      "\n",
      "         [[2.7613]],\n",
      "\n",
      "         [[0.5683]],\n",
      "\n",
      "         [[3.2674]],\n",
      "\n",
      "         [[0.2185]],\n",
      "\n",
      "         [[3.1502]],\n",
      "\n",
      "         [[2.5440]],\n",
      "\n",
      "         [[0.3508]],\n",
      "\n",
      "         [[0.3187]],\n",
      "\n",
      "         [[0.4334]],\n",
      "\n",
      "         [[0.1367]],\n",
      "\n",
      "         [[2.4522]],\n",
      "\n",
      "         [[0.1525]],\n",
      "\n",
      "         [[0.3358]],\n",
      "\n",
      "         [[0.0817]],\n",
      "\n",
      "         [[3.1967]],\n",
      "\n",
      "         [[0.3183]],\n",
      "\n",
      "         [[2.9865]],\n",
      "\n",
      "         [[2.2251]],\n",
      "\n",
      "         [[0.2311]],\n",
      "\n",
      "         [[3.1225]],\n",
      "\n",
      "         [[0.1414]],\n",
      "\n",
      "         [[0.1405]],\n",
      "\n",
      "         [[0.1339]],\n",
      "\n",
      "         [[0.1080]],\n",
      "\n",
      "         [[1.3979]],\n",
      "\n",
      "         [[0.1845]],\n",
      "\n",
      "         [[1.3565]],\n",
      "\n",
      "         [[1.0592]],\n",
      "\n",
      "         [[0.3598]],\n",
      "\n",
      "         [[0.3179]],\n",
      "\n",
      "         [[0.3386]],\n",
      "\n",
      "         [[3.1869]],\n",
      "\n",
      "         [[0.1327]],\n",
      "\n",
      "         [[0.1403]],\n",
      "\n",
      "         [[1.2477]],\n",
      "\n",
      "         [[0.1618]],\n",
      "\n",
      "         [[1.1901]],\n",
      "\n",
      "         [[1.3870]],\n",
      "\n",
      "         [[0.2055]],\n",
      "\n",
      "         [[0.2546]],\n",
      "\n",
      "         [[1.1929]],\n",
      "\n",
      "         [[3.0840]],\n",
      "\n",
      "         [[2.7173]],\n",
      "\n",
      "         [[0.1534]],\n",
      "\n",
      "         [[2.8499]],\n",
      "\n",
      "         [[2.9788]],\n",
      "\n",
      "         [[1.8772]],\n",
      "\n",
      "         [[0.3360]],\n",
      "\n",
      "         [[0.3430]],\n",
      "\n",
      "         [[0.9113]],\n",
      "\n",
      "         [[0.1325]],\n",
      "\n",
      "         [[2.3205]],\n",
      "\n",
      "         [[0.1188]],\n",
      "\n",
      "         [[1.2138]],\n",
      "\n",
      "         [[0.2076]],\n",
      "\n",
      "         [[2.4489]],\n",
      "\n",
      "         [[0.7946]],\n",
      "\n",
      "         [[0.1669]],\n",
      "\n",
      "         [[2.8784]],\n",
      "\n",
      "         [[0.8364]],\n",
      "\n",
      "         [[2.7361]],\n",
      "\n",
      "         [[0.3750]],\n",
      "\n",
      "         [[3.0244]],\n",
      "\n",
      "         [[0.4154]],\n",
      "\n",
      "         [[0.2126]],\n",
      "\n",
      "         [[0.2413]],\n",
      "\n",
      "         [[1.9424]],\n",
      "\n",
      "         [[0.2413]],\n",
      "\n",
      "         [[0.3188]],\n",
      "\n",
      "         [[2.8961]],\n",
      "\n",
      "         [[0.4717]],\n",
      "\n",
      "         [[0.6192]],\n",
      "\n",
      "         [[0.1307]],\n",
      "\n",
      "         [[2.6199]],\n",
      "\n",
      "         [[0.4495]],\n",
      "\n",
      "         [[2.6945]],\n",
      "\n",
      "         [[0.2135]],\n",
      "\n",
      "         [[0.3226]],\n",
      "\n",
      "         [[2.5361]],\n",
      "\n",
      "         [[0.0426]],\n",
      "\n",
      "         [[0.1475]],\n",
      "\n",
      "         [[0.2559]],\n",
      "\n",
      "         [[0.0257]],\n",
      "\n",
      "         [[2.8267]],\n",
      "\n",
      "         [[0.1620]],\n",
      "\n",
      "         [[0.1539]],\n",
      "\n",
      "         [[0.0333]],\n",
      "\n",
      "         [[0.0718]],\n",
      "\n",
      "         [[0.6059]],\n",
      "\n",
      "         [[0.3785]],\n",
      "\n",
      "         [[0.2842]],\n",
      "\n",
      "         [[2.2292]],\n",
      "\n",
      "         [[0.1308]],\n",
      "\n",
      "         [[0.0764]],\n",
      "\n",
      "         [[0.0838]],\n",
      "\n",
      "         [[0.2697]],\n",
      "\n",
      "         [[0.4902]],\n",
      "\n",
      "         [[0.1546]],\n",
      "\n",
      "         [[0.9783]],\n",
      "\n",
      "         [[0.2661]],\n",
      "\n",
      "         [[0.1998]],\n",
      "\n",
      "         [[0.2383]],\n",
      "\n",
      "         [[0.2479]],\n",
      "\n",
      "         [[0.1129]],\n",
      "\n",
      "         [[1.9494]],\n",
      "\n",
      "         [[1.2985]],\n",
      "\n",
      "         [[0.3007]],\n",
      "\n",
      "         [[2.8475]],\n",
      "\n",
      "         [[0.5777]],\n",
      "\n",
      "         [[0.2010]],\n",
      "\n",
      "         [[0.6114]],\n",
      "\n",
      "         [[0.8730]],\n",
      "\n",
      "         [[0.0577]],\n",
      "\n",
      "         [[2.6785]],\n",
      "\n",
      "         [[3.1324]],\n",
      "\n",
      "         [[0.1400]],\n",
      "\n",
      "         [[2.9146]],\n",
      "\n",
      "         [[3.2456]],\n",
      "\n",
      "         [[2.9944]],\n",
      "\n",
      "         [[0.1699]],\n",
      "\n",
      "         [[0.1643]],\n",
      "\n",
      "         [[0.0718]],\n",
      "\n",
      "         [[0.4031]],\n",
      "\n",
      "         [[1.2194]],\n",
      "\n",
      "         [[0.1093]],\n",
      "\n",
      "         [[0.1015]],\n",
      "\n",
      "         [[1.6197]],\n",
      "\n",
      "         [[3.1852]],\n",
      "\n",
      "         [[0.2539]],\n",
      "\n",
      "         [[2.9519]],\n",
      "\n",
      "         [[1.8116]],\n",
      "\n",
      "         [[0.1853]],\n",
      "\n",
      "         [[3.1380]],\n",
      "\n",
      "         [[0.2623]],\n",
      "\n",
      "         [[0.2089]],\n",
      "\n",
      "         [[0.3245]],\n",
      "\n",
      "         [[0.1092]],\n",
      "\n",
      "         [[2.8761]],\n",
      "\n",
      "         [[0.0297]],\n",
      "\n",
      "         [[2.3523]],\n",
      "\n",
      "         [[2.9740]],\n",
      "\n",
      "         [[1.2585]],\n",
      "\n",
      "         [[2.6449]],\n",
      "\n",
      "         [[0.0593]],\n",
      "\n",
      "         [[0.4534]],\n",
      "\n",
      "         [[0.1434]],\n",
      "\n",
      "         [[0.2023]],\n",
      "\n",
      "         [[1.6451]],\n",
      "\n",
      "         [[1.2657]],\n",
      "\n",
      "         [[2.7197]],\n",
      "\n",
      "         [[2.7539]],\n",
      "\n",
      "         [[2.3179]],\n",
      "\n",
      "         [[2.9051]],\n",
      "\n",
      "         [[0.1607]],\n",
      "\n",
      "         [[1.3182]],\n",
      "\n",
      "         [[3.1827]],\n",
      "\n",
      "         [[0.1013]],\n",
      "\n",
      "         [[0.2062]],\n",
      "\n",
      "         [[0.1325]],\n",
      "\n",
      "         [[0.0579]],\n",
      "\n",
      "         [[0.2779]],\n",
      "\n",
      "         [[2.8305]],\n",
      "\n",
      "         [[0.4055]],\n",
      "\n",
      "         [[0.2010]],\n",
      "\n",
      "         [[0.4500]],\n",
      "\n",
      "         [[1.0106]],\n",
      "\n",
      "         [[0.3143]],\n",
      "\n",
      "         [[2.7535]],\n",
      "\n",
      "         [[0.1352]],\n",
      "\n",
      "         [[0.2032]],\n",
      "\n",
      "         [[2.1164]],\n",
      "\n",
      "         [[0.6074]],\n",
      "\n",
      "         [[0.1854]],\n",
      "\n",
      "         [[0.1489]],\n",
      "\n",
      "         [[0.1269]],\n",
      "\n",
      "         [[0.1318]],\n",
      "\n",
      "         [[0.4362]],\n",
      "\n",
      "         [[2.9645]],\n",
      "\n",
      "         [[2.5869]],\n",
      "\n",
      "         [[0.0962]],\n",
      "\n",
      "         [[0.1815]],\n",
      "\n",
      "         [[1.4027]],\n",
      "\n",
      "         [[3.0700]],\n",
      "\n",
      "         [[0.0857]],\n",
      "\n",
      "         [[0.2116]],\n",
      "\n",
      "         [[0.4663]],\n",
      "\n",
      "         [[0.1554]],\n",
      "\n",
      "         [[2.7355]],\n",
      "\n",
      "         [[0.1516]],\n",
      "\n",
      "         [[0.2351]],\n",
      "\n",
      "         [[2.5931]],\n",
      "\n",
      "         [[1.7026]],\n",
      "\n",
      "         [[0.3800]],\n",
      "\n",
      "         [[2.7718]],\n",
      "\n",
      "         [[1.5151]],\n",
      "\n",
      "         [[0.2570]],\n",
      "\n",
      "         [[0.0772]],\n",
      "\n",
      "         [[0.5174]],\n",
      "\n",
      "         [[0.4512]],\n",
      "\n",
      "         [[0.5822]],\n",
      "\n",
      "         [[0.2476]],\n",
      "\n",
      "         [[3.0425]],\n",
      "\n",
      "         [[0.1710]],\n",
      "\n",
      "         [[0.5004]],\n",
      "\n",
      "         [[0.2342]],\n",
      "\n",
      "         [[2.8457]],\n",
      "\n",
      "         [[2.9485]],\n",
      "\n",
      "         [[0.2340]],\n",
      "\n",
      "         [[0.3189]],\n",
      "\n",
      "         [[2.6099]],\n",
      "\n",
      "         [[0.0416]],\n",
      "\n",
      "         [[0.1794]],\n",
      "\n",
      "         [[0.0880]],\n",
      "\n",
      "         [[1.9738]],\n",
      "\n",
      "         [[1.4902]],\n",
      "\n",
      "         [[0.4846]],\n",
      "\n",
      "         [[0.1855]],\n",
      "\n",
      "         [[0.5043]],\n",
      "\n",
      "         [[0.5405]],\n",
      "\n",
      "         [[3.0278]],\n",
      "\n",
      "         [[0.1453]],\n",
      "\n",
      "         [[0.1095]],\n",
      "\n",
      "         [[2.8529]],\n",
      "\n",
      "         [[3.0682]],\n",
      "\n",
      "         [[2.8265]],\n",
      "\n",
      "         [[0.0946]],\n",
      "\n",
      "         [[3.2208]],\n",
      "\n",
      "         [[0.5846]],\n",
      "\n",
      "         [[0.2840]],\n",
      "\n",
      "         [[1.2938]],\n",
      "\n",
      "         [[0.6300]],\n",
      "\n",
      "         [[2.5922]],\n",
      "\n",
      "         [[3.1850]],\n",
      "\n",
      "         [[1.9445]],\n",
      "\n",
      "         [[1.5217]],\n",
      "\n",
      "         [[0.0282]],\n",
      "\n",
      "         [[0.5756]],\n",
      "\n",
      "         [[2.0254]],\n",
      "\n",
      "         [[0.2331]],\n",
      "\n",
      "         [[2.7223]],\n",
      "\n",
      "         [[0.2998]],\n",
      "\n",
      "         [[2.0225]],\n",
      "\n",
      "         [[0.2069]],\n",
      "\n",
      "         [[0.1248]],\n",
      "\n",
      "         [[0.0495]],\n",
      "\n",
      "         [[2.6829]],\n",
      "\n",
      "         [[2.3955]],\n",
      "\n",
      "         [[0.4339]],\n",
      "\n",
      "         [[1.4189]],\n",
      "\n",
      "         [[2.6408]],\n",
      "\n",
      "         [[0.6525]],\n",
      "\n",
      "         [[0.1101]],\n",
      "\n",
      "         [[0.3400]],\n",
      "\n",
      "         [[0.0758]],\n",
      "\n",
      "         [[3.3618]],\n",
      "\n",
      "         [[0.2303]],\n",
      "\n",
      "         [[1.7265]],\n",
      "\n",
      "         [[0.2908]],\n",
      "\n",
      "         [[0.7883]],\n",
      "\n",
      "         [[0.2281]],\n",
      "\n",
      "         [[2.8828]],\n",
      "\n",
      "         [[0.5261]],\n",
      "\n",
      "         [[2.8860]],\n",
      "\n",
      "         [[3.1045]],\n",
      "\n",
      "         [[0.2200]],\n",
      "\n",
      "         [[1.2130]],\n",
      "\n",
      "         [[1.4881]],\n",
      "\n",
      "         [[0.1928]],\n",
      "\n",
      "         [[0.0660]],\n",
      "\n",
      "         [[0.0967]],\n",
      "\n",
      "         [[2.7290]],\n",
      "\n",
      "         [[0.1960]],\n",
      "\n",
      "         [[0.3217]],\n",
      "\n",
      "         [[0.2220]],\n",
      "\n",
      "         [[2.8740]],\n",
      "\n",
      "         [[1.3047]],\n",
      "\n",
      "         [[3.0873]],\n",
      "\n",
      "         [[0.1377]],\n",
      "\n",
      "         [[0.2750]],\n",
      "\n",
      "         [[2.6149]],\n",
      "\n",
      "         [[0.1417]],\n",
      "\n",
      "         [[0.1763]],\n",
      "\n",
      "         [[0.0914]],\n",
      "\n",
      "         [[2.6074]],\n",
      "\n",
      "         [[0.2552]],\n",
      "\n",
      "         [[0.0912]],\n",
      "\n",
      "         [[1.4314]],\n",
      "\n",
      "         [[0.2283]],\n",
      "\n",
      "         [[1.4544]],\n",
      "\n",
      "         [[0.1215]],\n",
      "\n",
      "         [[2.3567]],\n",
      "\n",
      "         [[0.1774]],\n",
      "\n",
      "         [[0.3957]],\n",
      "\n",
      "         [[0.5340]],\n",
      "\n",
      "         [[2.8732]],\n",
      "\n",
      "         [[0.0975]],\n",
      "\n",
      "         [[0.3948]],\n",
      "\n",
      "         [[2.9393]],\n",
      "\n",
      "         [[0.1197]],\n",
      "\n",
      "         [[0.7456]],\n",
      "\n",
      "         [[0.1528]],\n",
      "\n",
      "         [[0.2819]],\n",
      "\n",
      "         [[0.2539]],\n",
      "\n",
      "         [[0.2686]],\n",
      "\n",
      "         [[0.3766]],\n",
      "\n",
      "         [[0.2312]],\n",
      "\n",
      "         [[0.2014]],\n",
      "\n",
      "         [[0.0829]],\n",
      "\n",
      "         [[2.2967]],\n",
      "\n",
      "         [[3.0183]],\n",
      "\n",
      "         [[0.6363]],\n",
      "\n",
      "         [[3.0987]],\n",
      "\n",
      "         [[2.7683]],\n",
      "\n",
      "         [[2.4913]],\n",
      "\n",
      "         [[0.3129]],\n",
      "\n",
      "         [[0.3943]],\n",
      "\n",
      "         [[2.6180]],\n",
      "\n",
      "         [[0.3854]],\n",
      "\n",
      "         [[0.4154]],\n",
      "\n",
      "         [[0.1318]],\n",
      "\n",
      "         [[0.0985]],\n",
      "\n",
      "         [[2.8822]],\n",
      "\n",
      "         [[0.1903]],\n",
      "\n",
      "         [[0.3342]],\n",
      "\n",
      "         [[0.3060]],\n",
      "\n",
      "         [[0.3976]],\n",
      "\n",
      "         [[1.3235]],\n",
      "\n",
      "         [[0.1987]],\n",
      "\n",
      "         [[0.1275]],\n",
      "\n",
      "         [[0.3229]],\n",
      "\n",
      "         [[2.9467]],\n",
      "\n",
      "         [[2.5833]],\n",
      "\n",
      "         [[0.4956]],\n",
      "\n",
      "         [[0.1759]],\n",
      "\n",
      "         [[2.0665]],\n",
      "\n",
      "         [[2.8104]],\n",
      "\n",
      "         [[0.3382]],\n",
      "\n",
      "         [[1.1912]],\n",
      "\n",
      "         [[0.1937]],\n",
      "\n",
      "         [[2.7672]],\n",
      "\n",
      "         [[0.2671]],\n",
      "\n",
      "         [[0.3595]],\n",
      "\n",
      "         [[0.1816]],\n",
      "\n",
      "         [[2.4862]],\n",
      "\n",
      "         [[0.3299]],\n",
      "\n",
      "         [[1.2413]],\n",
      "\n",
      "         [[3.1024]],\n",
      "\n",
      "         [[0.2254]],\n",
      "\n",
      "         [[1.4926]],\n",
      "\n",
      "         [[2.5833]],\n",
      "\n",
      "         [[0.2794]],\n",
      "\n",
      "         [[0.2130]],\n",
      "\n",
      "         [[2.9302]],\n",
      "\n",
      "         [[2.7186]],\n",
      "\n",
      "         [[0.1122]],\n",
      "\n",
      "         [[0.1057]],\n",
      "\n",
      "         [[3.2715]],\n",
      "\n",
      "         [[1.7156]],\n",
      "\n",
      "         [[0.1810]],\n",
      "\n",
      "         [[0.0842]],\n",
      "\n",
      "         [[1.2549]],\n",
      "\n",
      "         [[0.4316]],\n",
      "\n",
      "         [[0.1951]],\n",
      "\n",
      "         [[0.4366]]]], device='cuda:0', grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### hook on avgpool!!!   it's tuple (x,0) after layer4,   or write a transform function\n",
    "target_layers=[model.model.avgpool]\n",
    "# target_layers=[model.model.layer4]\n",
    "input_tensor=data['image'][23].unsqueeze(0).to(device)\n",
    "cam = GradCAM(model=model, target_layers=target_layers, use_cuda=True)\n",
    "targets=[BinaryClassifierOutputTarget(1)]\n",
    "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "\n",
    "##(1, 512, 1, 1)\n",
    "# print(grayscale_cam.shape)\n",
    "# print(grayscale_cam.squeeze().shape)\n",
    "# print(np.squeeze(grayscale_cam,axis=0))\n",
    "# plt.figure()\n",
    "# # plt.imshow(grayscale_cam[0,:])\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae1c9264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[[1.0673, 1.5232, 1.6745,  ..., 1.2309, 1.0510, 0.7316],\n",
      "          [1.5212, 2.6046, 2.7841,  ..., 2.3073, 1.9133, 0.9317],\n",
      "          [2.1289, 2.3252, 2.9878,  ..., 3.3745, 3.2575, 1.0350],\n",
      "          ...,\n",
      "          [1.2466, 1.8353, 2.6468,  ..., 2.1743, 2.6406, 1.0694],\n",
      "          [1.0244, 1.3853, 1.5738,  ..., 1.4231, 1.4597, 0.9863],\n",
      "          [0.6307, 0.8020, 0.7642,  ..., 0.9066, 0.8160, 0.6168]],\n",
      "\n",
      "         [[1.4962, 1.3114, 2.4430,  ..., 1.9423, 1.4127, 0.8106],\n",
      "          [1.5064, 3.2966, 3.9181,  ..., 2.1270, 1.3749, 1.1978],\n",
      "          [1.5600, 3.7332, 4.8115,  ..., 3.3420, 1.8626, 1.4359],\n",
      "          ...,\n",
      "          [1.7994, 2.4683, 2.1365,  ..., 3.2343, 2.6170, 2.9455],\n",
      "          [2.4636, 2.9197, 1.8580,  ..., 2.9490, 1.6964, 2.4339],\n",
      "          [1.1967, 1.5096, 1.2778,  ..., 1.8087, 0.8726, 0.6054]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.4734, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.6007,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.6621, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.7106],\n",
      "          [0.5022, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.9897],\n",
      "          [0.4219, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.2173],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1954],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [2.5531, 2.3662, 2.7845,  ..., 2.6373, 2.0002, 3.0179]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2270, 0.4590],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.3118,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.6809, 0.5766, 1.0812],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.9260,  ..., 0.2141, 0.0000, 0.0244],\n",
      "          [0.1488, 1.1558, 1.8300,  ..., 0.9982, 0.2473, 1.2858]]]],\n",
      "       device='cuda:0', grad_fn=<ReluBackward0>), 0)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m cam \u001b[38;5;241m=\u001b[39m GradCAM(model\u001b[38;5;241m=\u001b[39mmodel, target_layers\u001b[38;5;241m=\u001b[39mtarget_layers, use_cuda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m targets\u001b[38;5;241m=\u001b[39m[BinaryClassifierOutputTarget(\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m----> 6\u001b[0m grayscale_cam \u001b[38;5;241m=\u001b[39m \u001b[43mcam\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\MPI\\lib\\site-packages\\pytorch_grad_cam\\base_cam.py:188\u001b[0m, in \u001b[0;36mBaseCAM.__call__\u001b[1;34m(self, input_tensor, targets, aug_smooth, eigen_smooth)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aug_smooth \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_augmentation_smoothing(\n\u001b[0;32m    186\u001b[0m         input_tensor, targets, eigen_smooth)\n\u001b[1;32m--> 188\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meigen_smooth\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\MPI\\lib\\site-packages\\pytorch_grad_cam\\base_cam.py:74\u001b[0m, in \u001b[0;36mBaseCAM.forward\u001b[1;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_input_gradient:\n\u001b[0;32m     71\u001b[0m     input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mVariable(input_tensor,\n\u001b[0;32m     72\u001b[0m                                            requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 74\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivations_and_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m     target_categories \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(outputs\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy(), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\MPI\\lib\\site-packages\\pytorch_grad_cam\\activations_and_gradients.py:43\u001b[0m, in \u001b[0;36mActivationsAndGradients.__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradients \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivations \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\MPI\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\thesis\\step5\\DL\\models\\Classifiers.py:111\u001b[0m, in \u001b[0;36mFFC_res18.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[1;32m--> 111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\MPI\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\thesis\\step5\\DL\\models\\model_zoo\\ffc_resnet.py:181\u001b[0m, in \u001b[0;36mFFCResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    179\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[0;32m    180\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n\u001b[1;32m--> 181\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    183\u001b[0m \u001b[38;5;66;03m#         x = self.avgpool(x)\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\MPI\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\MPI\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\MPI\\lib\\site-packages\\torch\\nn\\modules\\module.py:1547\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1545\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[0;32m   1546\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1547\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1550\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\MPI\\lib\\site-packages\\pytorch_grad_cam\\activations_and_gradients.py:25\u001b[0m, in \u001b[0;36mActivationsAndGradients.save_activation\u001b[1;34m(self, module, input, output)\u001b[0m\n\u001b[0;32m     23\u001b[0m     activation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreshape_transform(activation)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(activation)\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivations\u001b[38;5;241m.\u001b[39mappend(\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m()\u001b[38;5;241m.\u001b[39mdetach())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'cpu'"
     ]
    }
   ],
   "source": [
    "target_layers=[model.model.layer4[-1]]\n",
    "# target_layers=[model.model.layer4]\n",
    "input_tensor=data['image'][23].unsqueeze(0).to(device)\n",
    "cam = GradCAM(model=model, target_layers=target_layers, use_cuda=True)\n",
    "targets=[BinaryClassifierOutputTarget(1)]\n",
    "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d6383ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.2754, 0.0000]],\n",
      "\n",
      "         [[0.0790, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1274, 0.7816]],\n",
      "\n",
      "         [[0.1895, 0.5236]],\n",
      "\n",
      "         [[0.1025, 0.0000]]]], device='cuda:0', grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model_t = Res18()\n",
    "target_layers = [model_t.model.layer4[-1]]\n",
    "input_tensor = data['image'][23].unsqueeze(0).to(device)\n",
    "\n",
    "cam = GradCAM(model=model_t, target_layers=target_layers, use_cuda=True)\n",
    "\n",
    "# You can also use it within a with statement, to make sure it is freed,\n",
    "# In case you need to re-create it inside an outer loop:\n",
    "# with GradCAM(model=model, target_layers=target_layers, use_cuda=args.use_cuda) as cam:\n",
    "#   ...\n",
    "\n",
    "# We have to specify the target we want to generate\n",
    "# the Class Activation Maps for.\n",
    "# If targets is None, the highest scoring category\n",
    "# will be used for every image in the batch.\n",
    "# Here we use ClassifierOutputTarget, but you can define your own custom targets\n",
    "# That are, for example, combinations of categories, or specific outputs in a non standard model.\n",
    "\n",
    "targets=[BinaryClassifierOutputTarget(1)]\n",
    "\n",
    "# You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n",
    "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "37917e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 36)\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.02777777 0.08333332 0.13888887\n",
      "  0.19444442 0.24999996 0.3055555  0.36111104 0.4166666  0.47222215\n",
      "  0.52777773 0.58333325 0.63888884 0.69444436 0.74999994 0.8055555\n",
      "  0.86111104 0.9166666  0.9722221  0.9999998  0.9999998  0.9999998\n",
      "  0.9999998  0.9999998  0.9999998  0.9999998  0.9999998  0.9999998 ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.02777777 0.08333332 0.13888887\n",
      "  0.19444442 0.24999996 0.3055555  0.36111104 0.4166666  0.47222215\n",
      "  0.52777773 0.58333325 0.63888884 0.69444436 0.74999994 0.8055555\n",
      "  0.86111104 0.9166667  0.97222215 0.9999998  0.9999998  0.9999998\n",
      "  0.9999998  0.9999998  0.9999998  0.9999998  0.9999998  0.9999998 ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.02777777 0.08333332 0.13888887\n",
      "  0.19444442 0.24999996 0.3055555  0.36111104 0.4166666  0.47222215\n",
      "  0.52777773 0.58333325 0.63888884 0.69444436 0.74999994 0.8055555\n",
      "  0.86111104 0.91666657 0.9722221  0.9999998  0.9999998  0.9999998\n",
      "  0.9999998  0.9999998  0.9999998  0.9999998  0.9999998  0.9999998 ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.02777777 0.08333332 0.13888887\n",
      "  0.19444443 0.24999996 0.3055555  0.36111104 0.4166666  0.47222215\n",
      "  0.52777773 0.58333325 0.63888884 0.69444436 0.74999994 0.8055555\n",
      "  0.86111104 0.9166666  0.97222215 0.9999998  0.9999998  0.9999998\n",
      "  0.9999998  0.9999998  0.9999998  0.9999998  0.9999998  0.9999998 ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.02777777 0.08333332 0.13888887\n",
      "  0.19444442 0.24999997 0.3055555  0.36111104 0.4166666  0.47222215\n",
      "  0.52777773 0.58333325 0.63888884 0.69444436 0.74999994 0.8055555\n",
      "  0.86111104 0.9166666  0.9722221  0.9999999  0.9999999  0.9999999\n",
      "  0.9999999  0.9999999  0.9999999  0.9999999  0.9999999  0.9999999 ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.02777777 0.08333332 0.13888887\n",
      "  0.19444442 0.24999997 0.3055555  0.36111104 0.4166666  0.47222215\n",
      "  0.52777773 0.58333325 0.63888884 0.69444436 0.74999994 0.8055555\n",
      "  0.86111104 0.9166667  0.97222215 0.9999999  0.9999999  0.9999999\n",
      "  0.9999999  0.9999999  0.9999999  0.9999999  0.9999999  0.9999999 ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.02777777 0.08333332 0.13888887\n",
      "  0.19444442 0.24999997 0.3055555  0.36111104 0.4166666  0.47222215\n",
      "  0.52777773 0.58333325 0.63888884 0.69444436 0.74999994 0.8055555\n",
      "  0.86111104 0.9166666  0.9722221  0.9999999  0.9999999  0.9999999\n",
      "  0.9999999  0.9999999  0.9999999  0.9999999  0.9999999  0.9999999 ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.02777777 0.08333332 0.13888887\n",
      "  0.19444442 0.24999997 0.3055555  0.36111104 0.4166666  0.47222215\n",
      "  0.52777773 0.58333325 0.63888884 0.69444436 0.74999994 0.8055555\n",
      "  0.86111104 0.9166666  0.97222215 0.9999999  0.9999999  0.9999999\n",
      "  0.9999999  0.9999999  0.9999999  0.9999999  0.9999999  0.9999999 ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.02777777 0.08333332 0.13888887\n",
      "  0.19444442 0.24999997 0.3055555  0.36111104 0.41666663 0.47222215\n",
      "  0.52777773 0.58333325 0.63888884 0.69444436 0.74999994 0.8055555\n",
      "  0.86111104 0.9166666  0.97222215 0.9999999  0.9999999  0.9999999\n",
      "  0.9999999  0.9999999  0.9999999  0.9999999  0.9999999  0.9999999 ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.02777777 0.08333332 0.13888887\n",
      "  0.19444442 0.24999997 0.3055555  0.36111104 0.4166666  0.47222215\n",
      "  0.52777773 0.5833333  0.63888884 0.69444436 0.75       0.8055555\n",
      "  0.86111104 0.9166667  0.97222215 0.9999999  0.9999999  0.9999999\n",
      "  0.9999999  0.9999999  0.9999999  0.9999999  0.9999999  0.9999999 ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.02777777 0.08333332 0.13888887\n",
      "  0.19444442 0.24999997 0.3055555  0.36111104 0.41666663 0.47222215\n",
      "  0.52777773 0.58333325 0.63888884 0.69444436 0.74999994 0.8055555\n",
      "  0.86111104 0.9166667  0.97222215 0.9999999  0.9999999  0.9999999\n",
      "  0.9999999  0.9999999  0.9999999  0.9999999  0.9999999  0.9999999 ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.02777777 0.08333332 0.13888888\n",
      "  0.19444443 0.24999997 0.3055555  0.36111104 0.4166666  0.47222218\n",
      "  0.52777773 0.58333325 0.63888884 0.6944444  0.74999994 0.8055556\n",
      "  0.86111104 0.9166667  0.9722222  0.9999999  0.9999999  0.9999999\n",
      "  0.9999999  0.9999999  0.9999999  0.9999999  0.9999999  0.9999999 ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.02777777 0.08333332 0.13888887\n",
      "  0.1944444  0.24999996 0.3055555  0.36111104 0.41666657 0.47222215\n",
      "  0.52777773 0.58333325 0.63888884 0.69444436 0.7499999  0.8055555\n",
      "  0.86111104 0.91666657 0.9722221  0.9999998  0.9999998  0.9999998\n",
      "  0.9999998  0.9999998  0.9999998  0.9999998  0.9999998  0.9999998 ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.02777777 0.08333332 0.13888887\n",
      "  0.19444443 0.24999997 0.3055555  0.36111104 0.41666663 0.47222218\n",
      "  0.52777773 0.5833333  0.63888884 0.69444436 0.74999994 0.8055555\n",
      "  0.86111104 0.9166667  0.9722222  0.9999999  0.9999999  0.9999999\n",
      "  0.9999999  0.9999999  0.9999999  0.9999999  0.9999999  0.9999999 ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.02777777 0.08333332 0.13888887\n",
      "  0.1944444  0.24999996 0.30555546 0.36111104 0.4166666  0.47222215\n",
      "  0.5277777  0.5833332  0.63888884 0.69444436 0.7499999  0.8055555\n",
      "  0.86111104 0.91666657 0.9722221  0.9999998  0.9999998  0.9999998\n",
      "  0.9999998  0.9999998  0.9999998  0.9999998  0.9999998  0.9999998 ]]\n"
     ]
    }
   ],
   "source": [
    "grayscale_cam = grayscale_cam[0, :]\n",
    "print(grayscale_cam.shape)\n",
    "print(grayscale_cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "19e1fe6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x22487f67b50>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(grayscale_cam,cmap='gray')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2604b423",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
